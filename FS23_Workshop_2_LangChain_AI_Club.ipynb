{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AaryanNaruka14/AaryanNaruka14/blob/main/FS23_Workshop_2_LangChain_AI_Club.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop 2: Extending Language Models\n",
        "Building a system that can answer questions, using a document of your choice as a source.\n",
        "\n",
        "We'll be using **LangChain**\n",
        "\n",
        "\n",
        "\n",
        "## What is LangChain ?\n",
        "LangChain is an open-source framework for building applications powered by language models. It provides developers with tools to build applications using large language models (LLMs). LangChain allows developers to chain different prompts interactively.\n",
        "\n",
        "LangChain can be used to build applications that:\n",
        "- Connect a language model to other sources of context\n",
        "- Are context-aware\n",
        "- Integrate with external sources such as Google Drive, Notion, and Wikipedia\n"
      ],
      "metadata": {
        "id": "vrhH1PpQQWLQ"
      },
      "id": "vrhH1PpQQWLQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup: Install LangChain 🦜🔗\n",
        "\n",
        "Run the cell below. It will take about 7 minutes to run the first time."
      ],
      "metadata": {
        "id": "LYujbqTpQo-Y"
      },
      "id": "LYujbqTpQo-Y"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8c18c821-7d01-4f2e-8d68-c97605023430",
      "metadata": {
        "id": "8c18c821-7d01-4f2e-8d68-c97605023430",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d6de0e9f-735d-49ff-d040-29b0978f79dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain[all]\n",
            "  Downloading langchain-0.0.293-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured\n",
            "  Downloading unstructured-0.10.15-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain[all])\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain[all])\n",
            "  Downloading langsmith-0.0.38-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (8.2.3)\n",
            "Collecting O365<3.0.0,>=2.0.26 (from langchain[all])\n",
            "  Downloading O365-2.0.28-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.6/164.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aleph-alpha-client<3.0.0,>=2.15.0 (from langchain[all])\n",
            "  Downloading aleph_alpha_client-2.17.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting amadeus>=8.1.0 (from langchain[all])\n",
            "  Downloading amadeus-9.0.0.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting arxiv<2.0,>=1.4 (from langchain[all])\n",
            "  Downloading arxiv-1.4.8-py3-none-any.whl (12 kB)\n",
            "Collecting atlassian-python-api<4.0.0,>=3.36.0 (from langchain[all])\n",
            "  Downloading atlassian_python_api-3.41.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.2/167.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting awadb<0.4.0,>=0.3.9 (from langchain[all])\n",
            "  Downloading awadb-0.3.10-cp310-cp310-manylinux1_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-ai-formrecognizer<4.0.0,>=3.2.1 (from langchain[all])\n",
            "  Downloading azure_ai_formrecognizer-3.3.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.8/297.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-ai-vision<0.12.0,>=0.11.1b1 (from langchain[all])\n",
            "  Downloading azure_ai_vision-0.11.1b1-py3-none-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-cognitiveservices-speech<2.0.0,>=1.28.0 (from langchain[all])\n",
            "  Downloading azure_cognitiveservices_speech-1.32.1-py3-none-manylinux1_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-cosmos<5.0.0,>=4.4.0b1 (from langchain[all])\n",
            "  Downloading azure_cosmos-4.5.1-py3-none-any.whl (230 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.6/230.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-identity<2.0.0,>=1.12.0 (from langchain[all])\n",
            "  Downloading azure_identity-1.14.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.2/160.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5,>=4 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.11.2)\n",
            "Collecting clarifai>=9.1.0 (from langchain[all])\n",
            "  Downloading clarifai-9.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting clickhouse-connect<0.6.0,>=0.5.14 (from langchain[all])\n",
            "  Downloading clickhouse_connect-0.5.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.7/922.7 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere<5,>=4 (from langchain[all])\n",
            "  Downloading cohere-4.26.1-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deeplake<4.0.0,>=3.6.8 (from langchain[all])\n",
            "  Downloading deeplake-3.6.26.tar.gz (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.5/547.5 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docarray[hnswlib]<0.33.0,>=0.32.0 (from langchain[all])\n",
            "  Downloading docarray-0.32.1-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting duckduckgo-search<4.0.0,>=3.8.3 (from langchain[all])\n",
            "  Downloading duckduckgo_search-3.8.5-py3-none-any.whl (18 kB)\n",
            "Collecting elasticsearch<9,>=8 (from langchain[all])\n",
            "  Downloading elasticsearch-8.9.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.5/395.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting esprima<5.0.0,>=4.0.1 (from langchain[all])\n",
            "  Downloading esprima-4.0.1.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu<2,>=1 (from langchain[all])\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-python-client==2.70.0 (from langchain[all])\n",
            "  Downloading google_api_python_client-2.70.0-py2.py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<3.0.0,>=2.18.1 (from langchain[all])\n",
            "  Downloading google_auth-2.23.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.4/181.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-search-results<3,>=2 (from langchain[all])\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gptcache>=0.1.7 (from langchain[all])\n",
            "  Downloading gptcache-0.1.41-py3-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting html2text<2021.0.0,>=2020.1.16 (from langchain[all])\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Collecting huggingface_hub<1,>=0 (from langchain[all])\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2<4,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.1.2)\n",
            "Collecting jq<2.0.0,>=1.4.1 (from langchain[all])\n",
            "  Downloading jq-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (656 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m656.0/656.0 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lancedb<0.2,>=0.1 (from langchain[all])\n",
            "  Downloading lancedb-0.1.16-py3-none-any.whl (34 kB)\n",
            "Collecting langkit<0.1.0,>=0.0.6 (from langchain[all])\n",
            "  Downloading langkit-0.0.19-py3-none-any.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.1/769.1 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lark<2.0.0,>=1.1.5 (from langchain[all])\n",
            "  Downloading lark-1.1.7-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting libdeeplake<0.0.61,>=0.0.60 (from langchain[all])\n",
            "  Downloading libdeeplake-0.0.60-cp310-cp310-manylinux2010_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: librosa<0.11.0,>=0.10.0.post2 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (0.10.1)\n",
            "Requirement already satisfied: lxml<5.0.0,>=4.9.2 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.9.3)\n",
            "Collecting manifest-ml<0.0.2,>=0.0.1 (from langchain[all])\n",
            "  Downloading manifest_ml-0.0.1-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marqo<2.0.0,>=1.2.4 (from langchain[all])\n",
            "  Downloading marqo-1.3.1-py3-none-any.whl (36 kB)\n",
            "Collecting momento<2.0.0,>=1.5.0 (from langchain[all])\n",
            "  Downloading momento-1.9.2-py3-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.4/134.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nebula3-python<4.0.0,>=3.4.0 (from langchain[all])\n",
            "  Downloading nebula3_python-3.4.0-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.4/312.4 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting neo4j<6.0.0,>=5.8.1 (from langchain[all])\n",
            "  Downloading neo4j-5.12.0.tar.gz (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting networkx<3.0.0,>=2.6.3 (from langchain[all])\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nlpcloud<2,>=1 (from langchain[all])\n",
            "  Downloading nlpcloud-1.1.44-py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: nltk<4,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.8.1)\n",
            "Collecting nomic<2.0.0,>=1.0.43 (from langchain[all])\n",
            "  Downloading nomic-1.1.14.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai<1,>=0 (from langchain[all])\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openlm<0.0.6,>=0.0.5 (from langchain[all])\n",
            "  Downloading openlm-0.0.5-py3-none-any.whl (10 kB)\n",
            "Collecting opensearch-py<3.0.0,>=2.0.0 (from langchain[all])\n",
            "  Downloading opensearch_py-2.3.1-py2.py3-none-any.whl (327 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.3/327.3 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer-six<20221106,>=20221105 (from langchain[all])\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pexpect<5.0.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.8.0)\n",
            "Collecting pgvector<0.2.0,>=0.1.6 (from langchain[all])\n",
            "  Downloading pgvector-0.1.8-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pinecone-client<3,>=2 (from langchain[all])\n",
            "  Downloading pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pinecone-text<0.5.0,>=0.4.2 (from langchain[all])\n",
            "  Downloading pinecone_text-0.4.2-py3-none-any.whl (17 kB)\n",
            "Collecting psycopg2-binary<3.0.0,>=2.9.5 (from langchain[all])\n",
            "  Downloading psycopg2_binary-2.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymongo<5.0.0,>=4.3.3 (from langchain[all])\n",
            "  Downloading pymongo-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.3/671.3 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyowm<4.0.0,>=3.3.0 (from langchain[all])\n",
            "  Downloading pyowm-3.3.0-py3-none-any.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf<4.0.0,>=3.4.0 (from langchain[all])\n",
            "  Downloading pypdf-3.16.1-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytesseract<0.4.0,>=0.3.10 (from langchain[all])\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Collecting python-arango<8.0.0,>=7.5.9 (from langchain[all])\n",
            "  Downloading python_arango-7.6.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyvespa<0.34.0,>=0.33.0 (from langchain[all])\n",
            "  Downloading pyvespa-0.33.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting qdrant-client<2.0.0,>=1.3.1 (from langchain[all])\n",
            "  Downloading qdrant_client-1.5.4-py3-none-any.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdflib<7.0.0,>=6.3.2 (from langchain[all])\n",
            "  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting redis<5,>=4 (from langchain[all])\n",
            "  Downloading redis-4.6.0-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.1/241.1 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langchain[all])\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers<3,>=2 (from langchain[all])\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting singlestoredb<0.8.0,>=0.7.1 (from langchain[all])\n",
            "  Downloading singlestoredb-0.7.1-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.8/216.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-text<3.0.0,>=2.11.0 (from langchain[all])\n",
            "  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tigrisdb<2.0.0,>=1.0.0b6 (from langchain[all])\n",
            "  Downloading tigrisdb-1.0.0b6-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<0.4.0,>=0.3.2 (from langchain[all])\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (2.0.1+cu118)\n",
            "Collecting transformers<5,>=4 (from langchain[all])\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting weaviate-client<4,>=3 (from langchain[all])\n",
            "  Downloading weaviate_client-3.24.1-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia<2,>=1 (from langchain[all])\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wolframalpha==5.0.0 (from langchain[all])\n",
            "  Downloading wolframalpha-5.0.0-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==2.70.0->langchain[all]) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==2.70.0->langchain[all]) (0.1.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==2.70.0->langchain[all]) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==2.70.0->langchain[all]) (4.1.1)\n",
            "Collecting xmltodict (from wolframalpha==5.0.0->langchain[all])\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from wolframalpha==5.0.0->langchain[all]) (10.1.0)\n",
            "Collecting jaraco.context (from wolframalpha==5.0.0->langchain[all])\n",
            "  Downloading jaraco.context-4.3.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (1.3.1)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all]) (2.0.4)\n",
            "Collecting aiodns>=3.0.0 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
            "Collecting aiohttp-retry>=2.8.3 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading aiohttp_retry-2.8.3-py3-none-any.whl (9.8 kB)\n",
            "Collecting tokenizers>=0.13.2 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all]) (4.5.0)\n",
            "Requirement already satisfied: Pillow>=9.2.0 in /usr/local/lib/python3.10/dist-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all]) (9.4.0)\n",
            "Collecting feedparser (from arxiv<2.0,>=1.4->langchain[all])\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated (from atlassian-python-api<4.0.0,>=3.36.0->langchain[all])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from atlassian-python-api<4.0.0,>=3.36.0->langchain[all]) (1.16.0)\n",
            "Requirement already satisfied: oauthlib in /usr/local/lib/python3.10/dist-packages (from atlassian-python-api<4.0.0,>=3.36.0->langchain[all]) (3.2.2)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from atlassian-python-api<4.0.0,>=3.36.0->langchain[all]) (1.3.1)\n",
            "Collecting azure-core<2.0.0,>=1.23.0 (from azure-ai-formrecognizer<4.0.0,>=3.2.1->langchain[all])\n",
            "  Downloading azure_core-1.29.4-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msrest>=0.6.21 (from azure-ai-formrecognizer<4.0.0,>=3.2.1->langchain[all])\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-common~=1.1 (from azure-ai-formrecognizer<4.0.0,>=3.2.1->langchain[all])\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.10/dist-packages (from azure-identity<2.0.0,>=1.12.0->langchain[all]) (41.0.3)\n",
            "Collecting msal<2.0.0,>=1.20.0 (from azure-identity<2.0.0,>=1.12.0->langchain[all])\n",
            "  Downloading msal-1.24.0-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.3/91.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msal-extensions<2.0.0,>=0.3.0 (from azure-identity<2.0.0,>=1.12.0->langchain[all])\n",
            "  Downloading msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5,>=4->langchain[all]) (2.5)\n",
            "Collecting clarifai-grpc>=9.8.1 (from clarifai>=9.1.0->langchain[all])\n",
            "  Downloading clarifai_grpc-9.8.2-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.9/216.9 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tritonclient==2.34.0 (from clarifai>=9.1.0->langchain[all])\n",
            "  Downloading tritonclient-2.34.0-py3-none-manylinux1_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clarifai>=9.1.0->langchain[all]) (23.1)\n",
            "Collecting tqdm==4.64.1 (from clarifai>=9.1.0->langchain[all])\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich==13.4.2 (from clarifai>=9.1.0->langchain[all])\n",
            "  Downloading rich-13.4.2-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich==13.4.2->clarifai>=9.1.0->langchain[all]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich==13.4.2->clarifai>=9.1.0->langchain[all]) (2.16.1)\n",
            "Collecting python-rapidjson>=0.9.1 (from tritonclient==2.34.0->clarifai>=9.1.0->langchain[all])\n",
            "  Downloading python_rapidjson-1.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect<0.6.0,>=0.5.14->langchain[all]) (2023.7.22)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect<0.6.0,>=0.5.14->langchain[all]) (2023.3.post1)\n",
            "Collecting zstandard (from clickhouse-connect<0.6.0,>=0.5.14->langchain[all])\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect<0.6.0,>=0.5.14->langchain[all])\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff<3.0,>=2.0 (from cohere<5,>=4->langchain[all])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere<5,>=4->langchain[all])\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere<5,>=4->langchain[all]) (6.8.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain[all])\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain[all])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting boto3 (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading boto3-1.28.50-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake<4.0.0,>=3.6.8->langchain[all]) (8.1.7)\n",
            "Collecting pathos (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading pathos-0.3.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humbug>=0.3.1 (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Collecting numcodecs (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading numcodecs-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake<4.0.0,>=3.6.8->langchain[all]) (2.3.0)\n",
            "Collecting aioboto3>=10.4.0 (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading aioboto3-11.3.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake<4.0.0,>=3.6.8->langchain[all]) (1.5.7)\n",
            "Collecting orjson>=3.8.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading orjson-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-requests>=2.28.11.6 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading types_requests-2.31.0.2-py3-none-any.whl (14 kB)\n",
            "Collecting hnswlib>=0.6.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all]) (3.20.3)\n",
            "Collecting aiofiles>=23.1.0 (from duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting httpx[brotli,http2,socks]>=0.24.1 (from duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting elastic-transport<9,>=8 (from elasticsearch<9,>=8->langchain[all])\n",
            "  Downloading elastic_transport-8.4.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0,>=2.18.1->langchain[all]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0,>=2.18.1->langchain[all]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0,>=2.18.1->langchain[all]) (4.9)\n",
            "Collecting urllib3>=1.26 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[all]) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[all]) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4,>=3->langchain[all]) (2.1.3)\n",
            "Collecting pylance==0.5.10 (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading pylance-0.5.10-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ratelimiter (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
            "Collecting retry (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting attr (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading attr-0.3.2-py2.py3-none-any.whl (3.3 kB)\n",
            "Collecting semver (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading semver-3.0.1-py3-none-any.whl (17 kB)\n",
            "Collecting pyarrow>=10 (from pylance==0.5.10->lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from langkit<0.1.0,>=0.0.6->langchain[all]) (1.5.3)\n",
            "Collecting textstat<0.8.0,>=0.7.3 (from langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting whylogs==1.2.6 (from langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading whylogs-1.2.6-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4.0.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from whylogs==1.2.6->langkit<0.1.0,>=0.0.6->langchain[all]) (3.10.0)\n",
            "Collecting whylabs-client<1,>=0.5.1 (from whylogs==1.2.6->langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading whylabs_client-0.5.7-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.0/402.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting whylogs-sketching>=3.4.1.dev3 (from whylogs==1.2.6->langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading whylogs_sketching-3.4.1.dev3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.3/547.3 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.11.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.7.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.3.6)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.0.5)\n",
            "Collecting dill>=0.3.5 (from manifest-ml<0.0.2,>=0.0.1->langchain[all])\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlitedict>=2.0.0 (from manifest-ml<0.0.2,>=0.0.1->langchain[all])\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.46.0 in /usr/local/lib/python3.10/dist-packages (from momento<2.0.0,>=1.5.0->langchain[all]) (1.57.0)\n",
            "Collecting momento-wire-types<0.76.0,>=0.75.0 (from momento<2.0.0,>=1.5.0->langchain[all])\n",
            "  Downloading momento_wire_types-0.75.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjwt (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: future>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from nebula3-python<4.0.0,>=3.4.0->langchain[all]) (0.18.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4,>=3->langchain[all]) (2023.6.3)\n",
            "Collecting jsonlines (from nomic<2.0.0,>=1.0.43->langchain[all])\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting loguru (from nomic<2.0.0,>=1.0.43->langchain[all])\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wonderwords (from nomic<2.0.0,>=1.0.43->langchain[all])\n",
            "  Downloading wonderwords-2.2.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from O365<3.0.0,>=2.0.26->langchain[all]) (2.8.2)\n",
            "Collecting tzlocal<5.0,>=4.0 (from O365<3.0.0,>=2.0.26->langchain[all])\n",
            "  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting stringcase>=1.2.0 (from O365<3.0.0,>=2.0.26->langchain[all])\n",
            "  Downloading stringcase-1.2.0.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect<5.0.0,>=4.8.0->langchain[all]) (0.7.0)\n",
            "Collecting dnspython>=2.0.0 (from pinecone-client<3,>=2->langchain[all])\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmh3<4.0.0,>=3.1.0 (from pinecone-text<0.5.0,>=0.4.2->langchain[all])\n",
            "  Downloading mmh3-3.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38 kB)\n",
            "Collecting torch<3,>=1 (from langchain[all])\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wget<4.0,>=3.2 (from pinecone-text<0.5.0,>=0.4.2->langchain[all])\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting geojson<3,>=2.3.0 (from pyowm<4.0.0,>=3.3.0->langchain[all])\n",
            "  Downloading geojson-2.5.0-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PySocks<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from pyowm<4.0.0,>=3.3.0->langchain[all]) (1.7.1)\n",
            "Requirement already satisfied: setuptools>=42 in /usr/local/lib/python3.10/dist-packages (from python-arango<8.0.0,>=7.5.9->langchain[all]) (67.7.2)\n",
            "Collecting docker (from pyvespa<0.34.0,>=0.33.0->langchain[all])\n",
            "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.3.1->langchain[all])\n",
            "  Downloading grpcio_tools-1.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.3.1->langchain[all])\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib<7.0.0,>=6.3.2->langchain[all])\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib<7.0.0,>=6.3.2->langchain[all]) (3.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[all]) (3.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3,>=2->langchain[all]) (0.15.2+cu118)\n",
            "Collecting sentencepiece (from sentence-transformers<3,>=2->langchain[all])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from singlestoredb<0.8.0,>=0.7.1->langchain[all]) (1.0.3)\n",
            "Collecting sqlparams (from singlestoredb<0.8.0,>=0.7.1->langchain[all])\n",
            "  Downloading sqlparams-5.1.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from singlestoredb<0.8.0,>=0.7.1->langchain[all]) (0.41.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain[all]) (2.0.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.14.0)\n",
            "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (2.13.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<3,>=1->langchain[all])\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<3,>=1->langchain[all])\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<3,>=1->langchain[all])\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<3,>=1->langchain[all])\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5,>=4->langchain[all])\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting validators<1.0.0,>=0.21.2 (from weaviate-client<4,>=3->langchain[all])\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting authlib<2.0.0,>=1.2.1 (from weaviate-client<4,>=3->langchain[all])\n",
            "  Downloading Authlib-1.2.1-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiobotocore[boto3]==2.6.0 (from aioboto3>=10.4.0->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading aiobotocore-2.6.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.31.18,>=1.31.17 (from aiobotocore[boto3]==2.6.0->aioboto3>=10.4.0->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.6.0->aioboto3>=10.4.0->deeplake<4.0.0,>=3.6.8->langchain[all]) (1.15.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore[boto3]==2.6.0->aioboto3>=10.4.0->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting boto3 (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading boto3-1.28.17-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycares>=4.0.0 (from aiodns>=3.0.0->aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading pycares-4.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.7/288.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos>=1.53.0 in /usr/local/lib/python3.10/dist-packages (from clarifai-grpc>=9.8.1->clarifai>=9.1.0->langchain[all]) (1.60.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.12.0->langchain[all]) (1.15.1)\n",
            "Collecting protobuf>=3.19.0 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading protobuf-4.24.3-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio<2.0.0,>=1.46.0 (from momento<2.0.0,>=1.5.0->langchain[all])\n",
            "  Downloading grpcio-1.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.19.0,>=0.18.0 (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search<4.0.0,>=3.8.3->langchain[all]) (1.3.0)\n",
            "Collecting brotli (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting socksio==1.* (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Collecting h2<5,>=3 (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere<5,>=4->langchain[all]) (3.16.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.39.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.18.1->langchain[all]) (0.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa<0.11.0,>=0.10.0.post2->langchain[all]) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (2.3.0)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting msrest>=0.6.21 (from azure-ai-formrecognizer<4.0.0,>=3.2.1->langchain[all])\n",
            "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-core<2.0.0,>=1.23.0 (from azure-ai-formrecognizer<4.0.0,>=3.2.1->langchain[all])\n",
            "  Downloading azure_core-1.29.3-py3-none-any.whl (191 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.4/191.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading azure_core-1.29.2-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.6/188.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading azure_core-1.29.1-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.33.0)\n",
            "Collecting pyphen (from textstat<0.8.0,>=0.7.3->langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-urllib3 (from types-requests>=2.28.11.6->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain[all])\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pytz-deprecation-shim (from tzlocal<5.0,>=4.0->O365<3.0.0,>=2.0.26->langchain[all])\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->singlestoredb<0.8.0,>=0.7.1->langchain[all]) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->singlestoredb<0.8.0,>=0.7.1->langchain[all]) (2.0.1)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyvespa<0.34.0,>=0.33.0->langchain[all]) (1.6.2)\n",
            "Collecting sgmllib3k (from feedparser->arxiv<2.0,>=1.4->langchain[all])\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from numcodecs->deeplake<4.0.0,>=3.6.8->langchain[all]) (0.4)\n",
            "Collecting ppft>=1.7.6.7 (from pathos->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading ppft-1.7.6.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.3 (from pathos->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading pox-0.3.3-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.15 (from pathos->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py<2.0.0,>=1.4.26 (from retry->lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from sentence-transformers<3,>=2->langchain[all])\n",
            "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.12.0->langchain[all]) (2.21)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.19.0,>=0.18.0->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search<4.0.0,>=3.8.3->langchain[all]) (3.7.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore<0.19.0,>=0.18.0->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich==13.4.2->clarifai>=9.1.0->langchain[all]) (0.1.2)\n",
            "INFO: pip is looking at multiple versions of pyjwt[crypto] to determine which version is compatible with other requirements. This could take a while.\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (2.3.7)\n",
            "Collecting tzdata (from pytz-deprecation-shim->tzlocal<5.0,>=4.0->O365<3.0.0,>=2.0.26->langchain[all])\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.19.0,>=0.18.0->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search<4.0.0,>=3.8.3->langchain[all]) (1.1.3)\n",
            "Building wheels for collected packages: amadeus, deeplake, esprima, google-search-results, neo4j, nomic, sentence-transformers, wikipedia, hnswlib, sqlitedict, stringcase, wget, sgmllib3k\n",
            "  Building wheel for amadeus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for amadeus: filename=amadeus-9.0.0-py2.py3-none-any.whl size=75047 sha256=3e58fd511993f4c1e3f3655cd9a4308879ecd0e8d710e0b86b0dbccd559dec8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/50/ea/3417d93eee6760a945d7711333d8d42b9f482e84600ef7f711\n",
            "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.6.26-py3-none-any.whl size=660905 sha256=77fb17c545489f1932e391ceaf848ec6f81235c3899011546cc55436a0d27e75\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/2e/77/5a105f65959e2e22ef87ef416efc64e4a5e954a84e4b232456\n",
            "  Building wheel for esprima (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for esprima: filename=esprima-4.0.1-py3-none-any.whl size=62240 sha256=c4f22596cb8c5653bd9f928ab8ca8018e8ebd785a13d57e853424be3a979c0a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/ad/8b/afd6e521e6aaea5482b7b4665ff3ce5a92373bd285e7d3a85c\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32003 sha256=de190a655eaa73ac53f67c3d6e6ed34ceeaa47bd1313471a72b997b048748d45\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neo4j: filename=neo4j-5.12.0-py3-none-any.whl size=263745 sha256=8855873f271832ab2adf38d287fe5318f78503e2123b53944a0ed510f5aa25dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/ea/49/d6abec5b49a566a80395ebb8e9744352e6512ab15ce0c8f40a\n",
            "  Building wheel for nomic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nomic: filename=nomic-1.1.14-py3-none-any.whl size=33822 sha256=94038a77526358a60b1a149b4cbafbcc62ffcee7366e68ce188c3f19582973d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/dd/9a/57a82068ce36ce73954949a52ebaa1745441728b8b0233421e\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=500a3b5bb26699df8c74922f5b537d03bda5a987f8ade82b59204e06df74f51d\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=67c5839b0a41f0fecbf5e9577469637395e7013c3aa93b7634631fed598ffc79\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2202681 sha256=b98af30a8caf3fc36f7ae86025b0b505f1699f689ba932a64e8c693915e3786b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=d85e90d55ae362dd452dfa663fd84a8092e0c41590608943f29d111ca1f6621c\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "  Building wheel for stringcase (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stringcase: filename=stringcase-1.2.0-py3-none-any.whl size=3568 sha256=203794851bf20ed592532657ba68d74745923ae9b7721df95241ed1cf1a8d3f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/ba/22/1a2d952a9ce8aa86e42fda41e2c87fdaf20e238c88bf8df013\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=383787e05862ad224de09171a99f29f3da8bc0adde1cb5a195fef032cabcede1\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=6e482eca389ba255b6478e599e09c9a03ccfaef941bba1ae62e069dac4f64ce2\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built amadeus deeplake esprima google-search-results neo4j nomic sentence-transformers wikipedia hnswlib sqlitedict stringcase wget sgmllib3k\n",
            "Installing collected packages: whylogs-sketching, wget, types-urllib3, tokenizers, stringcase, sqlitedict, sgmllib3k, sentencepiece, safetensors, ratelimiter, mmh3, geojson, filetype, faiss-cpu, esprima, brotli, azure-common, attr, zstandard, xmltodict, wonderwords, validators, urllib3, tzdata, types-requests, tqdm, sqlparams, socksio, semver, redis, python-rapidjson, python-magic, pytesseract, pyphen, pypdf, pyjwt, pyarrow, py, psycopg2-binary, protobuf, ppft, pox, portalocker, pgvector, orjson, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numcodecs, networkx, neo4j, mypy-extensions, marshmallow, lz4, loguru, libdeeplake, lark, jsonlines, jq, jmespath, jaraco.context, isodate, hyperframe, html2text, hpack, hnswlib, h11, grpcio, feedparser, fastavro, emoji, dnspython, dill, deprecated, backoff, azure-cognitiveservices-speech, azure-ai-vision, awadb, amadeus, aioitertools, aiofiles, wolframalpha, whylabs-client, typing-inspect, tritonclient, textstat, rich, retry, rdflib, pytz-deprecation-shim, pymongo, pylance, pycares, nvidia-cudnn-cu11, nebula3-python, multiprocess, momento-wire-types, httpcore, h2, grpcio-tools, google-auth, elastic-transport, clickhouse-connect, botocore, arxiv, wikipedia, whylogs, tzlocal, torch, tiktoken, tigrisdb, singlestoredb, s3transfer, requests-toolbelt, pinecone-client, pdfminer-six, pathos, opensearch-py, openlm, openai, nlpcloud, momento, marqo, manifest-ml, langsmith, lancedb, humbug, huggingface_hub, httpx, gptcache, google-search-results, elasticsearch, docker, docarray, dataclasses-json, cohere, clarifai-grpc, azure-core, authlib, aiohttp-retry, aiodns, aiobotocore, weaviate-client, unstructured, transformers, torchvision, pyvespa, python-arango, pyowm, O365, nomic, msrest, msal, langkit, langchain, google-api-python-client, clarifai, boto3, azure-cosmos, atlassian-python-api, aleph-alpha-client, sentence-transformers, qdrant-client, msal-extensions, duckduckgo-search, azure-ai-formrecognizer, pinecone-text, azure-identity, aioboto3, tensorflow-text, deeplake\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: pyjwt\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 9.0.0\n",
            "    Uninstalling pyarrow-9.0.0:\n",
            "      Successfully uninstalled pyarrow-9.0.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.1\n",
            "    Uninstalling networkx-3.1:\n",
            "      Successfully uninstalled networkx-3.1\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.57.0\n",
            "    Uninstalling grpcio-1.57.0:\n",
            "      Successfully uninstalled grpcio-1.57.0\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.5.2\n",
            "    Uninstalling rich-13.5.2:\n",
            "      Successfully uninstalled rich-13.5.2\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.17.3\n",
            "    Uninstalling google-auth-2.17.3:\n",
            "      Successfully uninstalled google-auth-2.17.3\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.0.1\n",
            "    Uninstalling tzlocal-5.0.1:\n",
            "      Successfully uninstalled tzlocal-5.0.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.84.0\n",
            "    Uninstalling google-api-python-client-2.84.0:\n",
            "      Successfully uninstalled google-api-python-client-2.84.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.17.3, but you have google-auth 2.23.0 which is incompatible.\n",
            "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 13.0.0 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.24.3 which is incompatible.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed O365-2.0.28 aioboto3-11.3.0 aiobotocore-2.6.0 aiodns-3.0.0 aiofiles-23.2.1 aiohttp-retry-2.8.3 aioitertools-0.11.0 aleph-alpha-client-2.17.0 amadeus-9.0.0 arxiv-1.4.8 atlassian-python-api-3.41.2 attr-0.3.2 authlib-1.2.1 awadb-0.3.10 azure-ai-formrecognizer-3.3.0 azure-ai-vision-0.11.1b1 azure-cognitiveservices-speech-1.32.1 azure-common-1.1.28 azure-core-1.29.1 azure-cosmos-4.5.1 azure-identity-1.14.0 backoff-2.2.1 boto3-1.28.17 botocore-1.31.17 brotli-1.1.0 clarifai-9.8.1 clarifai-grpc-9.8.2 clickhouse-connect-0.5.25 cohere-4.26.1 dataclasses-json-0.5.14 deeplake-3.6.26 deprecated-1.2.14 dill-0.3.7 dnspython-2.4.2 docarray-0.32.1 docker-6.1.3 duckduckgo-search-3.8.5 elastic-transport-8.4.0 elasticsearch-8.9.0 emoji-2.8.0 esprima-4.0.1 faiss-cpu-1.7.4 fastavro-1.8.2 feedparser-6.0.10 filetype-1.2.0 geojson-2.5.0 google-api-python-client-2.70.0 google-auth-2.23.0 google-search-results-2.4.2 gptcache-0.1.41 grpcio-1.58.0 grpcio-tools-1.58.0 h11-0.14.0 h2-4.1.0 hnswlib-0.7.0 hpack-4.0.0 html2text-2020.1.16 httpcore-0.18.0 httpx-0.25.0 huggingface_hub-0.17.2 humbug-0.3.2 hyperframe-6.0.1 isodate-0.6.1 jaraco.context-4.3.0 jmespath-1.0.1 jq-1.6.0 jsonlines-4.0.0 lancedb-0.1.16 langchain-0.0.293 langkit-0.0.19 langsmith-0.0.38 lark-1.1.7 libdeeplake-0.0.60 loguru-0.7.2 lz4-4.3.2 manifest-ml-0.0.1 marqo-1.3.1 marshmallow-3.20.1 mmh3-3.1.0 momento-1.9.2 momento-wire-types-0.75.0 msal-1.24.0 msal-extensions-1.0.0 msrest-0.7.1 multiprocess-0.70.15 mypy-extensions-1.0.0 nebula3-python-3.4.0 neo4j-5.12.0 networkx-2.8.8 nlpcloud-1.1.44 nomic-1.1.14 numcodecs-0.11.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 openai-0.28.0 openlm-0.0.5 opensearch-py-2.3.1 orjson-3.9.7 pathos-0.3.1 pdfminer-six-20221105 pgvector-0.1.8 pinecone-client-2.2.4 pinecone-text-0.4.2 portalocker-2.8.2 pox-0.3.3 ppft-1.7.6.7 protobuf-4.24.3 psycopg2-binary-2.9.7 py-1.11.0 pyarrow-13.0.0 pycares-4.3.0 pyjwt-2.8.0 pylance-0.5.10 pymongo-4.5.0 pyowm-3.3.0 pypdf-3.16.1 pyphen-0.14.0 pytesseract-0.3.10 python-arango-7.6.2 python-magic-0.4.27 python-rapidjson-1.11 pytz-deprecation-shim-0.1.0.post0 pyvespa-0.33.0 qdrant-client-1.5.4 ratelimiter-1.2.0.post0 rdflib-6.3.2 redis-4.6.0 requests-toolbelt-1.0.0 retry-0.9.2 rich-13.4.2 s3transfer-0.6.2 safetensors-0.3.3 semver-3.0.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 sgmllib3k-1.0.0 singlestoredb-0.7.1 socksio-1.0.0 sqlitedict-2.1.0 sqlparams-5.1.0 stringcase-1.2.0 tensorflow-text-2.13.0 textstat-0.7.3 tigrisdb-1.0.0b6 tiktoken-0.3.3 tokenizers-0.13.3 torch-1.13.1 torchvision-0.14.1 tqdm-4.64.1 transformers-4.33.2 tritonclient-2.34.0 types-requests-2.31.0.2 types-urllib3-1.26.25.14 typing-inspect-0.9.0 tzdata-2023.3 tzlocal-4.3.1 unstructured-0.10.15 urllib3-1.26.16 validators-0.22.0 weaviate-client-3.24.1 wget-3.2 whylabs-client-0.5.7 whylogs-1.2.6 whylogs-sketching-3.4.1.dev3 wikipedia-1.4.0 wolframalpha-5.0.0 wonderwords-2.2.0 xmltodict-0.13.0 zstandard-0.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.3\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.10-py3-none-any.whl (422 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.4/422.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.12)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<0.100.0,>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.13.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm>=4.65.0 (from chromadb)\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.24.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (1.26.16)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.3)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=cfad68739ef738026a7b4340f8053169a741a33ed7e1c8d3c94d6d1cf00c51e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, websockets, uvloop, uvicorn, tqdm, python-dotenv, pulsar-client, overrides, humanfriendly, httptools, chroma-hnswlib, bcrypt, watchfiles, starlette, posthog, coloredlogs, onnxruntime, fastapi, chromadb\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "clarifai 9.8.1 requires tqdm==4.64.1, but you have tqdm 4.66.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bcrypt-4.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.10 coloredlogs-15.0.1 fastapi-0.99.1 httptools-0.6.0 humanfriendly-10.0 monotonic-1.6 onnxruntime-1.15.1 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 tqdm-4.66.1 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.20.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "# the [all] refers to downloading the required dependencies along with LangChain itself\n",
        "!pip install langchain[all] unstructured\n",
        "!pip install pdf2image\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the **LangChain** library with all the dependencies"
      ],
      "metadata": {
        "id": "q_ghr82Evdod"
      },
      "id": "q_ghr82Evdod"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "87d5407d-ff3d-4b7e-94f5-dd1c12d721df",
      "metadata": {
        "id": "87d5407d-ff3d-4b7e-94f5-dd1c12d721df"
      },
      "outputs": [],
      "source": [
        "# Loading all the required functions for building the QA system\n",
        "from langchain.document_loaders import WebBaseLoader, PDFMinerLoader\n",
        "from langchain.document_loaders import OnlinePDFLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Your OpenAI API Key\n",
        "The key will be used to access ChatGPT from the notebook.\n",
        "\n",
        "During the workshop, we'll provide you with a temporary key.\n",
        "\n",
        "To create a key, follow these steps"
      ],
      "metadata": {
        "id": "PM8ZK2sB0To2"
      },
      "id": "PM8ZK2sB0To2"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4cee1083-211f-4312-82e6-4e739e34f935",
      "metadata": {
        "id": "4cee1083-211f-4312-82e6-4e739e34f935"
      },
      "outputs": [],
      "source": [
        "# This is a temporary key\n",
        "# If you're trying this on your own, you'll want to make your own API key (this one will stop working after the workshop)\n",
        "OPENAI_API_KEY=\"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Data Preparation\n",
        "\n",
        "## 1A: Getting the Text\n",
        "You can download the data from an online PDF, such as an academic paper, using LangCHain's `OnlinePDFLoader`"
      ],
      "metadata": {
        "id": "8UnVu-s60mNS"
      },
      "id": "8UnVu-s60mNS"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a76e06e9-2c4c-4256-9b25-ee3577207326",
      "metadata": {
        "id": "a76e06e9-2c4c-4256-9b25-ee3577207326"
      },
      "outputs": [],
      "source": [
        "# Defining a Variable to store the file path or article link you want to do question answering on.\n",
        "# webpage_path = 'https://www.macworld.com/article/2059274/wonderlust-keynote-script-iphone-15-apple-watch-series-9-airpods.html'\n",
        "webpage_path = 'https://arxiv.org/pdf/2211.12588.pdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "47d6e4c5-2bd0-48be-be37-3ef86342cb7e",
      "metadata": {
        "id": "47d6e4c5-2bd0-48be-be37-3ef86342cb7e"
      },
      "outputs": [],
      "source": [
        "# Defining a Loader object\n",
        "# The loader object is specifc to the type of file you selected in the previous page.\n",
        "# For webpage there is WebBaseLoader, for online pdfs (arxiv articles) there is OnlinePDFLoader\n",
        "\n",
        "# loader = WebBaseLoader(webpage_path)\n",
        "loader = OnlinePDFLoader(webpage_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f83dc9d4-9a93-4aca-9a5a-04e54d9691d0",
      "metadata": {
        "id": "f83dc9d4-9a93-4aca-9a5a-04e54d9691d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31f53dc-a470-4175-de71-1cd2fc59b1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# loader.load() starts to extract text/information from the provided file link.\n",
        "# So if you are doing question answering on an Article.\n",
        "# loader.load() step will extract all the text from that article link.\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see what the output looks like\n",
        "print(data)"
      ],
      "metadata": {
        "id": "cVXI-ycSSxmi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596d3dcf-db0c-487d-920d-1d665490c0db"
      },
      "id": "cVXI-ycSSxmi",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='University of Waterloo Vector Institute, Toronto University of California, Santa Barabra Google Research {wenhuchen,x93ma}@uwaterloo.ca, xinyi_wang@ucsb.edu, wcohen@google.com\\n\\nProgram of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks ♦ ∗ , Xueguang Ma\\n\\n♠,♣\\n\\n∗ , Wenhu Chen\\n\\n♦\\n\\n♠\\n\\n♣\\n\\n♠\\n\\n♥\\n\\nXinyi Wang,\\n\\n♥\\n\\nWilliam W. Cohen\\n\\n2 2 0 2\\n\\nv o N 9 2\\n\\n] L C . s c [\\n\\n3 v 8 8 5 2 1 . 1 1 2 2 : v i X r a\\n\\nAbstract\\n\\nRecently, there has been signiﬁcant progress in teaching language models to perform step- by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is the state-of-art method for many of these tasks. CoT uses language models to produce text describing reasoning, and com- putation, and ﬁnally the answer to a ques- tion. Here we propose ‘Program of Thoughts’ (PoT), which uses language models (mainly Codex) to generate text and programming lan- guage statements, and ﬁnally an answer. In PoT, the computation can be delegated to a pro- gram interpreter, which is used to execute the generated program, thus decoupling complex computation from reasoning and language un- derstanding. We evaluate PoT on ﬁve math word problem datasets and three ﬁnancial-QA datasets in both few-shot and zero-shot set- tings. We ﬁnd that PoT has an average perfor- mance gain over CoT of around 12% across all datasets. By combining PoT with self- consistency decoding, we can achieve SoTA performance on all the math datasets and near- SoTA performance on the ﬁnancial datasets. All of our data and code are released in Github1.\\n\\n1\\n\\nIntroduction\\n\\nNumerical reasoning is a long-standing task in arti- ﬁcial intelligence. A surge of datasets has been pro- posed recently to benchmark deep-learning mod- els’ capabilities to perform numerical/arithmetic reasoning. Some widely used benchmarks are based on Math word problems (MWP) (Cobbe et al., 2021; Patel et al., 2021; Lu et al., 2022; Ling et al., 2017), where systems are supposed to an- swer math questions expressed with natural text.\\n\\nBesides MWP, some datasets also consider ﬁnan- cial problems (Chen et al., 2021b, 2022; Zhu et al., 2021), where systems need to answer math-driven ﬁnancial questions.\\n\\nPrior work (Ling et al., 2017; Cobbe et al., 2021) has studied how to train models from scratch or ﬁne-tune models to generate intermediate steps to derive the ﬁnal answer. Such methods are data- intensive, requiring a signiﬁcant number of train- ing examples with expert-annotated steps. Re- cently, Wei et al. (2022) have discovered that the large language models (LLMs) (Brown et al., 2020; Chen et al., 2021a; Chowdhery et al., 2022) can be prompted with a few input-output exemplars to solve these tasks without any training or ﬁne-tuning. In particular, when prompted with a few examples containing inputs, natural language ‘rationales’, and outputs, LLMs can imitate the demonstrations to both generate rationales and answer these ques- tions. Such a prompting method is dubbed ‘Chain of Thoughts (CoT)’, and it is able to achieve state- of-the-art performance on a wide spectrum of textu and numerical reasoning datasets.\\n\\nCoT uses LLMs for both reasoning and com- putation, i.e. the language model not only needs to generate the mathematical expressions but also needs to perform the math in each step. We ar- gue that language models are not ideal for actually solving these mathematical expressions, because: 1) LLMs are very prone to arithmetic calculation errors, especially when dealing with large num- bers; 2) LLMs cannot solve complex mathematical expressions like polynomial equations or even dif- ferential equations; 3) LLMs are highly inefﬁcient at expressing iteration, especially when the number of iteration steps is large.\\n\\nWork done at University of Waterloo. Wenhu Chen is responsible for paper writing and running Math QA experi- ments, and Xueguang Ma is responsible for running ﬁnancial QA experiments and paper polishing.\\n\\n1https://github.com/wenhuchen/\\n\\n∗\\n\\nProgram-of-Thoughts\\n\\nIn order to solve these issues, we propose program-of-thoughts (PoT) prompting, which will delegate computation steps to an external language interpreter. In PoT, LMs can express reasoning steps as Python programs, and the computation can\\n\\nFigure 1: Comparison between Chain of Thoughts and Program of Thoughts.\\n\\nbe accomplished by a Python interpreter. We depict the difference between CoT and PoT in Figure 1. In the upper example, for CoT the iteration runs for 50 times, which leads to extremely low accu- racy;2 in the lower example, CoT cannot solve the cubic equation with language models and outputs a wrong answer. In contrast, in the upper example, PoT can express the iteration process with a few lines of code, which can be executed on a Python interpreter to derive an accurate answer; and in the lower example, PoT can convert the problem into a program that relies on ‘SymPy’ library in Python to solve the complex equation.\\n\\nWe evaluate PoT prompting across ﬁve MWP datasets, GSM8K, AQuA, SVAMP, TabMWP, Mul- tiArith; and three ﬁnancial datasets, FinQA, Con- vFinQA, and TATQA. These datasets cover various input formats including text, tables, and conversa- tion. We give an overview of the results in Figure 2. Under both few-shot and zero-shot settings, PoT outperforms CoT signiﬁcantly across all the eval- uated datasets. Under the few-shot setting, the average gain over CoT is around 8% for the MWP datasets and 15% for the ﬁnancial datasets. Under the zero-shot setting, the average gain over CoT is around 12% for the MWP datasets. PoT com- bined with self-consistency (SC) also outperforms\\n\\n2Assuming each addition is correct with 90% chance, after 50 additions, the likelihood of a correct output is less than 1%.\\n\\nCoT+SC (Wang et al., 2022b) by an average of 10% across all datasets. Our PoT+SC achieves the best- known results on all the evaluated MWP datasets and near SoTA results on the ﬁnancial datasets. Fi- nally, we conduct comprehensive ablation studies to understand the different components of PoT.\\n\\n2 Program of Thoughts\\n\\n2.1 Preliminaries\\n\\nIn-context learning has been described in (Brown et al., 2020; Chen et al., 2021a; Chowdhery et al., 2022; Rae et al., 2021). Compared with ﬁne-tuning, in-context learning (1) only takes a few annotation- s/demonstrations as a prompt, and (2) performs inference without training the model parameters. With in-context learning, LLMs receive the input- output exemplars as the preﬁx, followed by an input problem, and generate outputs imitating the exem- plars. More recently, ‘chain of thoughts prompt- ing’ (Wei et al., 2022) has been proposed as a spe- ciﬁc type of in-context learning where the exem- plar’s output contains the ‘thought process’ or ra- tionale instead of just an output. This approach has been shown to elicit LLMs’ reasoning capabilities.\\n\\n2.2 Program of Thoughts\\n\\nBesides natural language, programs can also be used to express our thought processes. By using se- mantically meaningful variable names, a program\\n\\nCoT\\n\\nPoT\\n\\n85.2\\n\\nthe problem.\\n\\n71.6\\n\\n76.4\\n\\n73.2\\n\\n63.1\\n\\n65.2\\n\\n64.5\\n\\n64.6\\n\\n54.1\\n\\n45.3\\n\\n40.4\\n\\n45.5\\n\\nG S M 8 K\\n\\nT a CoT-SC PoT-SC\\n\\nA Q u A\\n\\nS V A M P\\n\\nb M W P\\n\\nF i\\n\\nn Q A\\n\\nC o\\n\\nn\\n\\nv F i\\n\\nn\\n\\n78\\n\\n80\\n\\n86.8\\n\\n89.1\\n\\n81.8\\n\\n75.4\\n\\n68.1\\n\\n67.3\\n\\n58.6\\n\\n52\\n\\n44.4\\n\\n47.9\\n\\nG S M 8 K ZS-CoT\\n\\nA Q u A\\n\\nS V A M P ZS-PoT\\n\\nT a\\n\\nb M W P\\n\\nF i\\n\\nn Q A\\n\\nC o\\n\\nn\\n\\nv F i\\n\\nn\\n\\n70.8\\n\\n63.7\\n\\n65.2\\n\\n57\\n\\n53.5\\n\\n40.5\\n\\n43.9\\n\\n69\\n\\n61.4\\n\\nT A T Q A\\n\\n73.4\\n\\n63.9\\n\\nT A T Q A\\n\\n92.2\\n\\n79.3\\n\\nUnlike CoT, PoT relegates some computation to an external process (a Python interpreter). The LLMs are only responsible for expressing the ‘rea- soning process’ in the programming language. In contrast, CoT aims to use LLMs to perform both reasoning and computation. We argue that such an approach is more expressive and accurate.\\n\\nThe ‘program of thoughts’ is different from gen- erating equations directly, where the generation 3 − 2000 − target would be solve(20000 ∗ (1 + x) x∗20000∗3−1000, x). As observed by (Wei et al., 2022) for CoT, directly generating such equations is challenging for LLMs. PoT differs from equation generation in two aspects: (1) PoT breaks down the equation into a multi-step ‘thought’ process, and (2) PoT binds semantic meanings to variables to help ground the model in language. We found that this sort of ‘thoughtful’ process can elicit lan- guage models’ reasoning capabilities and generate more accurate programs. We provide a detailed comparison in the experimental section.\\n\\n31.9\\n\\nGSM8K\\n\\nAQuA\\n\\nSVAMP\\n\\nTabMWP MultiArith\\n\\n2.3 PoT Prompting\\n\\nFigure 2: Few-shot (upper), Few-shot + SC (middle) and Zero-Shot (lower) Performance overview of Codex PoT and Codex CoT across different datasets.\\n\\nFigure 3: Prompting LLMs to generate outputs with few-shot exemplars.\\n\\nWe show the proposed PoT prompting method in Figure 4 under the few-shot and zero-shot set- tings. Under the few-shot setting, a few exemplars of (question, ‘program of thoughts’) pairs will be preﬁxed as demonstrations to teach the LLM how to generate ‘thoughtful’ programs. Under the zero- shot setting, the prompt only contains an instruction without any exemplar demonstration. Unlike zero- shot CoT (Kojima et al., 2022), which requires an extra step to extract the answer from the ‘chain of thoughts’, zero-shot PoT can return the answer straightforwardly without extra steps.\\n\\n2.4 PoT as an Intermediate Step\\n\\ncan also be a natural representation to convey hu- man thoughts. For example, in the lower example in Figure 1, we ﬁrst create an unknown variable named interest_rate. Then we bind ‘summation in two years with ... interest rate’ to the variable sum_in_two_years_with_XXX_interest and write down the equation expressing their math- ematical relations with interest_rate. These equations are packaged into the ‘solve’ function provided by ‘SymPy’. The program is executed with Python to solve the equations to derive the variable interest_rate, which is the answer to\\n\\nFor certain problems requiring additional common- sense reasoning, we need to ﬁrst utilize PoT to gen- erate a program to compute an intermediate result, which is combined with the question to continue prompting LLM to derive the ﬁnal answer. For instance, in the left example in Figure 4, the pro- gram will generate a ﬂoat number 2.05. However, adding 2.05 to 11 AM cannot be easily handled by a Python program. Therefore, we continue to prompt the LLM to perform an additional step of textual reasoning to derive the ﬁnal answer. We adopt this approach for the AQuA dataset.\\n\\nFigure 4: Left: Few-shot PoT prompting, Right: Zero-shot PoT prompting.\\n\\n3 Experiments\\n\\n3.1 Experimental Setup\\n\\nDatasets We summarize our evaluated datasets in Table 1. We use the test set for all the evaluated datasets except TATQA. These datasets are highly heterogeneous in terms of their input formats. We conduct comprehensive experiments on this broad spectrum of datasets to show the generalizability and applicability of PoT prompting.\\n\\nTo incorporate the diverse inputs, we propose to linearize these inputs in the prompt. For table inputs, we adopt the same strategy as Chen (2022) to linearize a table into a text string. The columns of the table are separated by ‘|’ and the rows are separated by ‘\\\\n’. If a table cell is empty, it is ﬁlled by ’-’. For text+table hybrid inputs, we separate tables and text with ‘\\\\n’. For conversational his- tory, we also separate conversation turns by ‘\\\\n’. The prompt is constructed by the concatenation of task instruction, text, linearized table, and question. For conversational question answering, we simply concatenate all the dialog history in the prompt.\\n\\nImplementation Details We use the OpenAI Codex (code-davinci-002) API3 and GPT-3 (text- davinci-002) API4 model in our experiments. We use Python 3.8 with the SymPy library5 to execute the generated program. For the few-shot setting, we use 4-8 shots for all the datasets, based on their difﬁculty. For simple datasets like FinQA (Chen et al., 2021b), we tend to use less shots, while for\\n\\nmore challenging datasets like AQuA (Ling et al., 2017) and TATQA (Zhu et al., 2021), we use 8 shots to cover more diverse problems. The exam- ples are taken from the training set. We generally write prompts for 10-20 examples, and then tune the exemplar selection on a small validation set to choose the best 4-8 shots for the full set evaluation. To elicit the LLM’s capability to perform multi- step reasoning, we use the text “Let’s write a Python program step by step\" as our prompt. How- ever, a caveat is that LLM can fall back to gener- ating a reasoning chain in comments rather than in program. Therefore, we suppress the ‘#’ token logits by -2 to decrease its probability to avoid such cases. We found that this simple strategy can greatly improve performance.\\n\\nMetrics We adopt exact match scores as our eval- uation metrics for GSM8K, SVAMP, and Multi- Arith datasets. We will round the predicted number to a speciﬁc precision and then compare it with the reference number. For the AQuA dataset, we use PoT to compute the intermediate answer and then prompt the LLM again to output the closest option to measure the accuracy. For TabMWP, Con- vFinQA, and TATQA datasets, we use the ofﬁcial evaluation scripts provided on Github. For FinQA, we relax the evaluation for CoT because LLMs can- not perform the computation precisely (especially with high-precision ﬂoats and large numbers), so we adopt ‘math.isclose’ with relative tolerance of 0.001 to compare answers.\\n\\n3https://openai.com/blog/openai-codex/ 4https://beta.openai.com/ 5https://www.sympy.org/en/index.html\\n\\nBaselines We report results for three different models including Codex (Chen et al., 2021a), GPT-\\n\\nDataset\\n\\nSplit\\n\\nExample Domain\\n\\nInput\\n\\nOutput\\n\\nGSM8K (Cobbe et al., 2021) AQuA (Ling et al., 2017) SVAMP (Patel et al., 2021) MultiArith (Roy and Roth, 2015) TabMWP (Lu et al., 2022) FinQA (Chen et al., 2021b) ConvFinQA (Chen et al., 2022) TATQA (Zhu et al., 2021)\\n\\nTest Test Test Test Test Test Test Dev\\n\\n1318 253 1000 600 7861 1147 421 1668\\n\\nMWP MWP MWP MWP MWP Finance Finance Finance\\n\\nNumber Question Option Question Number Question Number Question Number + Text Table + Question Table + Text + Question Number + Binary Table + Text + Multi-Turn Question Number + Binary Table + Text + Question\\n\\nNumber + Text\\n\\nTable 1: Summarization of all the datasets being evaluated.\\n\\n3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022) and LaMDA (Thoppilan et al., 2022). We consider two types of prediction strategies includ- ing direct answer output and chain of thought to derive the answer. Since PaLM API is not public, we only list PaLM results reported from previous work (Wei et al., 2022; Wang et al., 2022b). We also leverage an external calculator as suggested in Wei et al. (2022) for all the equations generated by CoT, which is denoted as CoT + calc. Besides greedy decoding, we use self-consistency (Wang et al., 2022b) with CoT, taking majority vote over 40 different completions as the prediction.\\n\\n3.2 Main Results\\n\\nFew-shot Results We give our few-shot results in Table 2. On MWP datasets, PoT with greedy decoding improves on GSM8K/AQuA/TabMWP by more than 8%. On SVAMP, the improvement is 4% mainly due to its simplicity. For ﬁnancial QA datasets, PoT improves over CoT by roughly 20% on FinQA/ConvFinQA and 8% on TATQA. The larger improvements in FinQA and ConvFinQA are mainly due to miscalculations on LLMs for large numbers (e.g. in the millions). CoT adopts LLMs to perform the computation, which is highly prone to miscalculation errors, while PoT adopts a highly precise external computer to solve the problem. As an ablation, we also compare with CoT+calc, which leverages an external calculator to correct the calculation results in the generated ‘chain of thoughts’. The experiments show that adding an external calculator only shows mild improvement over CoT on MWP datasets, much behind PoT.\\n\\nFew-shot + Self-Consistency Results We lever- age self-consistency (SC) decoding to understand the upper bound of our method. This sampling- based decoding algorithm can greatly reduce ran- domness in the generation procedure and boosts performance. Speciﬁcally, we set a temperature of\\n\\n0.4 and K=40 throughout our experiments. Accord- ing to to Table 2, we found that PoT + SC still out- performs CoT + SC on MWP datasets with notable margins. On ﬁnancial datasets, we observe that self-consistency decoding is less impactful for both PoT and CoT. Similarly, PoT + SC outperforms CoT + SC by roughly 20% on FinQA/ConvFinQA, and 7% on TATQA.\\n\\nZero-shot Results We also evaluate the zero- shot performance of PoT and compare with Ko- jima et al. (2022) in Table 3. As can be seen, zero- shot PoT signiﬁcantly outperforms zero-shot CoT across all the MWP datasets evaluated. Compared to few-shot prompting, zero-shot PoT outperforms zero-shot CoT (Kojima et al., 2022) by an even larger margin. On the evaluated datasets, PoT’s out- performs CoT by an average of 12%. On TabMWP, zero-shot PoT is even higher than few-shot CoT. These results show the great potential to directly generalize to many unseen numerical tasks even without any dataset-speciﬁc exemplars.\\n\\n3.3 Ablation Studies\\n\\nWe performed multiple ablation studies under the few-shot setting to understand the importance of different factors in PoT.\\n\\nBackend GPT-3 vs. Codex We evaluated GPT- 3 (text-davinci-002) with PoT prompting. Un- like Codex, GPT-3 is not optimized for generating programs, and one would expect degraded perfor- mance with GPT-3 as the LLM. We choose three datasets, GSM8K, SVAMP, and FinQA, to analyze the performance difference of PoT and compare that relative to CoT. We show our experimental results in Table 4. We can see that the gap be- tween Codex and GPT-3 with PoT is consistently smaller than their gap with CoT. We conclude that our prompting approach is still effective for mod- els that are not speciﬁcally optimized for program generation. However, we do observe that the gap\\n\\nModel\\n\\n#Params GSM8K AQuA SVAMP\\n\\nTabWMP\\n\\nFinQA ConvFinQA TATQA Avg\\n\\nFine-tuned or few-shot prompt\\n\\nPublished SoTA\\n\\n\\n\\n78.0\\n\\n52.0\\n\\n86.8\\n\\n68.2\\n\\n68.0\\n\\n68.9\\n\\n73.6\\n\\n70.7\\n\\nFew-shot prompt (Greedy Decoding)\\n\\nCodex Direct Codex CoT GPT-3 Direct GPT-3 CoT PaLM Direct PaLM CoT\\n\\n175B 175B 175B 175B 540B 540B\\n\\n19.7 63.1 15.6 46.9 17.9 56.9\\n\\n29.5 45.3 24.8 35.8 25.2 35.8\\n\\n69.9 76.4 65.7 68.9 69.4 79.0\\n\\n59.4 65.2 57.1 62.9 - -\\n\\n25.6 40.4 14.4 26.1 - -\\n\\n40.0 45.6 29.1 37.4 - -\\n\\n55.0 61.4 37.9 42.5 - -\\n\\n42.7 56.7 34.9 45.7 - -\\n\\nCodex CoTcalc GPT-3 CoTcalc PaLM CoTcalc\\n\\n175B 175B 540B\\n\\n65.4 49.6 58.6\\n\\n45.3 35.8 35.8\\n\\n77.0 70.3 79.8\\n\\n65.8 63.4 -\\n\\n- -\\n\\n- -\\n\\n- -\\n\\n- -\\n\\nPoT (Ours)\\n\\n175B\\n\\n71.6\\n\\n54.1\\n\\n85.2\\n\\n73.2\\n\\n64.5\\n\\n64.6\\n\\n69.0\\n\\n68.9\\n\\nFew-shot prompt (Self-Consistency Decoding)\\n\\nLaMDA CoT-SC 137B 175B Codex CoT-SC 540B PaLM CoT-SC\\n\\nPoT-SC (Ours)\\n\\n175B\\n\\n27.7 78.0 74.4\\n\\n80.0\\n\\n26.8 52.0 48.3\\n\\n58.6\\n\\n53.5 86.8 86.6\\n\\n89.1\\n\\n75.4 - 81.8\\n\\n44.4 - 68.1\\n\\n47.9 -\\n\\n67.3\\n\\n63.2 - 70.2\\n\\n63.9 -\\n\\n73.6\\n\\nTable 2: The few-shot results for different datasets. Published SoTA includes the best-known results. On GSM8K, AQuA and SVAMP, the prior SoTA results are CoT + self-consistency decoding (Wang et al., 2022b). On FinQA, the prior best result is from Wang et al. (2022a). On ConvFinQA, the prior best result is achieved by Fin- QANet (Chen et al., 2022). On TabWMP (Lu et al., 2022), the prior best result is achieved by Dynamic Prompt Learning (Lu et al., 2022). On TATQA, the SoTA result is achieved by RegHNT (Lei et al., 2022).\\n\\nModel\\n\\n#Params GSM8K AQuA SVAMP\\n\\nTabMWP MultiArith Avg\\n\\nZero-shot Direct (GPT-3) Zero-shot CoT (GPT-3) Zero-shot CoT (PaLM)\\n\\n150B 150B 540B\\n\\n12.6 40.5 43.0\\n\\n22.4 31.9 -\\n\\n58.7 63.7 -\\n\\n38.9 53.5 -\\n\\n22.7 79.3 66.1\\n\\n31.0 53.7 -\\n\\nZero-shot PoT (Ours)\\n\\n150B\\n\\n57.0\\n\\n43.9\\n\\n70.8\\n\\n66.5\\n\\n92.2\\n\\n66.1\\n\\nTable 3: The zero-shot results for different datasets. The baseline results are taken from Kojima et al. (2022).\\n\\nModel\\n\\nGSM8K SVAMP\\n\\nFinQA\\n\\nv1\\n\\nv2\\n\\nv2\\n\\nCodex CoT GPT3 CoT Codex - GPT3 (CoT)\\n\\nCodex PoT GPT3 PoT Codex - GPT3 (PoT)\\n\\n63.1 46.9 +16.2\\n\\n71.6 60.4 +11.2\\n\\n76.4 58.9 +7.5\\n\\n85.2 80.1 +5.1\\n\\n40.4 26.1 +14.3\\n\\n64.5 56.7 +7.8\\n\\n0.62\\n\\n0.58\\n\\n0.55\\n\\n0.65\\n\\n0.68\\n\\n0.6\\n\\n0.67\\n\\n0.69\\n\\n0.65\\n\\n0.73\\n\\n0.7\\n\\n0.74\\n\\nTable 4: GPT-3 and Codex performance difference un- der CoT and PoT prompting.\\n\\n2-shots\\n\\n0.63\\n\\n0.64\\n\\n4-shots\\n\\n0.66\\n\\n0.63\\n\\n0.62\\n\\n6-shots\\n\\n0.66\\n\\n0.64\\n\\n0.62\\n\\n8-shots\\n\\n0.64\\n\\n0.62\\n\\n0.62\\n\\nincreases as the dataset becomes more challenging.\\n\\n0.58\\n\\nSensitivity to Exemplars To better understand how sensitive PoT is w.r.t different exemplars, we conduct a sensitivity analysis. Speciﬁcally, we wrote 20 total exemplars. For k-shot learning, we randomly sample k = (2, 4, 6, 8) out of the 20 ex- emplars three times as v1, v2, and v3. We will use these randomly sampled exemplars as demonstra-\\n\\n2-shots\\n\\n4-shots\\n\\n6-shots\\n\\n8-shots\\n\\nFigure 5: Exemplar sensitivity analysis for GSM8K and FinQA, where v1, v2 and v3 are three versions of k-shot demonstration sampled from the pool.\\n\\nMethod\\n\\nGSM8K SVAMP\\n\\nFinQA\\n\\nPoT CoT\\n\\n86\\n\\n88\\n\\nPoT PoT - Binding PoT - MultiStep\\n\\n71.6 60.2 45.8\\n\\n85.2 83.8 81.9\\n\\n64.5 61.6 58.9\\n\\n50\\n\\n66\\n\\n56\\n\\n50\\n\\n40\\n\\n72\\n\\n62\\n\\n65\\n\\n40\\n\\n38\\n\\n40\\n\\nTable 5: Comparison between PoT and equation gener- ation on three different datasets.\\n\\n10\\n\\n20\\n\\n20\\n\\ntions for PoT. We summarize our sensitivity analy- sis in Figure 5. First of all, we found that increasing the number of shots helps more for GSM8K than FinQA. This is mainly due to the diversity of ques- tions in GSM8K. By adding more exemplars, the language models can better generalize to diverse questions. Another observation is that when given fewer exemplars, PoT’s performance variance is larger. When K=2, the performance variance can be as large as 7% for both datasets. With more exemplars, the performance becomes more stable.\\n\\ngeometric\\n\\npolynomial\\n\\nsymbolic\\n\\nnumerical combinatorics\\n\\nequation linear\\n\\niterative\\n\\nprobability\\n\\nFigure 6: PoT and CoT’s breakdown accuracy across different types of questions.\\n\\n‘symbolic’, and ‘combinatorics’. These questions require more complex arithmetic or symbolic skills to solve. In contrast, on ‘numerical’, ‘probability’, and ‘geometric’ questions, PoT and CoT perform similarly.\\n\\nSemantic Binding and Multi-Step Reasoning The two core properties of ‘program of thoughts’ are: (1) multiple steps: breaking down the thought process into the step-by-step program, (2) semantic binding: associating semantic meaning to the vari- able names. To better understand how these two properties contribute, we compared with two vari- ants. One variant is to remove the semantic binding and simply use a, b, c as the variable names. The other variant is to directly predict the ﬁnal mathe- matical equation to compute the results. We show our ﬁndings in Table 5. As can be seen, removing the binding will in general hurt the model’s per- formance. On more complex questions involving more variables like GSM8K, the performance drop is larger. Similarly, prompting LLMs to directly generate the target equations is also very challeng- ing. Breaking down the target equation into mul- tiple reasoning steps helps boost the performance.\\n\\nBreakdown Analysis We perform further anal- ysis to determine which kinds of problems CoT and PoT differ most in performance. We use AQuA (Ling et al., 2017) as our testbed for this. Speciﬁcally, we manually classify the questions in AQuA into several categories and show the accu- racy on each subcategory in Figure 6. The major categories are (1) linear equations, (2) numerical calculation, (3) combinatorics, (4) probability, and (5) iterative.\\n\\nThe largest improvements of PoT are in the cat- egories ‘linear/polynomial equation’, ‘iterative’,\\n\\nError Analysis We considered two types of er- rors: (1) value grounding error, and (2) logic gener- ation error. The ﬁrst type indicates that the model fails to assign correct values to the variables rel- evant to the question. The second type indicates that the model fails to generate the correct com- putation process to answer the question based on the deﬁned variables. Figure 7 shows an exam- ple of each type of error. In the upper example, the model fetches the value of the variables incor- rectly while the computation logic is correct. In the lower example, the model grounded relevant variables correctly but fails to generate proper com- putation logic to answer the question. We manually examined the errors made in the TAT-QA results. Among the 198 failure cases of numerical reason- ing question with the PoT (greedy) method, 47% have value grounding errors and 33% have logic errors. In 15% both types of errors occurred and in 5% we believe the answer is actually correct. We found that the majority of the errors are value grounding errors, which is also common for other methods such as CoT.\\n\\n4 Related Work\\n\\n4.1 Mathematical Reasoning in NLP\\n\\nMathematical reasoning skills are essential for general-purpose intelligent systems, which have attracted a signiﬁcant amount of attention from the community. Earlier, there have been studies in un- derstanding NLP models’ capabilities to solve arith- metic/algebraic questions (Hosseini et al., 2014;\\n\\nFigure 7: Error cases on TAT-QA dev set using PoT-greedy method.\\n\\nKoncel-Kedziorski et al., 2015; Roy and Roth, 2015; Ling et al., 2017; Roy and Roth, 2018). Re- cently, more challenging datasets (Dua et al., 2019; Saxton et al., 2019; Miao et al., 2020; Amini et al., 2019; Hendrycks et al., 2021; Patel et al., 2021) have been proposed to increase the difﬁculty, di- versity or even adversarial robustness. One con- current work (published within 20 days) similar to ours is LiLA (Mishra et al., 2022), which proposes to assemble a large set of mathematical datasets into a uniﬁed dataset. LiLA also annotates Python programs as the generation target for solving math- ematical problems. However, LiLA is mostly fo- cused on dataset uniﬁcation. Our work aims to understand how to generate ‘thoughtful programs’ to best elicit LLM’s reasoning capability. Besides, we also investigate how to solve math problems without any exemplars.\\n\\n4.2\\n\\nIn-context Learning with LLMs\\n\\nGPT-3 (Brown et al., 2020) demonstrated a strong capability to perform few-shot predictions, where the model is given a description of the task in natu- ral language with few examples. Scaling model size, data, and computing are crucial to enable this learning ability. Recently, (Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Du et al., 2022) have proposed to train different types of LLMs with different training recipes. The ca-\\n\\npability to follow few-shot exemplars to solve un- seen tasks is not existent on smaller LMs, but only emerge as the model scales up (Kaplan et al., 2020). Recently, there have been several works (Xie et al., 2021; Min et al., 2022) aiming to understand how and why in-context learning works. Another con- current work similar to ours is BINDER (Cheng et al., 2022), which applies Codex to synthesize ‘soft’ SQL queries to answer questions from tables.\\n\\n4.3 Chain of Reasoning with LLMs\\n\\nAlthough LLMs have demonstrated remarkable success across a range of NLP tasks, their abil- ity to reason is often seen as a limitation. Recently, CoT (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022b) was proposed to enable LLM’s ca- pability to perform reasoning tasks by demonstrat- ing ‘natural language rationales’. Suzgun et al. (2022) have shown that CoT can already surpass hu- man performance on challenging BIG-Bench tasks. Later on, several other works (Drozdov et al., 2022; Zhou et al., 2022; Nye et al., 2021) also propose different approaches to utilize LLMs to solve com- positional reasoning tasks by allowing intermediate steps.\\n\\n5 Conclusions\\n\\nIn this work, we investigate how to disentangle computation from reasoning in solving numerical\\n\\nproblems. By ‘program of thoughts’ prompting, we are able to elicit LLMs’ abilities to generate accurate programs to express complex reasoning, while also allowing computation to be separately handled by an external program interpreter. This approach is able to boost the state-of-the-art perfor- mance on several math datasets signiﬁcantly. We believe our work can inspire more work to combine symbolic execution with LLMs.\\n\\nReferences\\n\\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha- jishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based for- malisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 2357–2367.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.\\n\\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\\n\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar- ian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training veri- ﬁers to solve math word problems. arXiv preprint arXiv:2110.14168.\\n\\nAndrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. 2022. Compositional semantic parsing with large language models. arXiv preprint arXiv:2209.15003.\\n\\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Efﬁcient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547–5569. PMLR.\\n\\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: A reading comprehension benchmark requir- ing discrete reasoning over paragraphs. In Proceed- ings of the 2019 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368–2378.\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Evaluating large lan- Brockman, et al. 2021a. arXiv preprint guage models trained on code. arXiv:2107.03374.\\n\\nWenhu Chen. 2022.\\n\\nfew (1)-shot arXiv:2210.06710.\\n\\ntable reasoners.\\n\\nLarge language models are arXiv preprint\\n\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical arXiv problem solving with the math dataset. preprint arXiv:2103.03874.\\n\\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb catego- rization. In EMNLP, pages 523–533.\\n\\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge, et al. 2021b. Finqa: A dataset of numerical reason- ing over ﬁnancial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan- guage Processing, pages 3697–3711.\\n\\nZhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. Con- vﬁnqa: Exploring the chain of numerical reasoning in conversational ﬁnance question answering. arXiv preprint arXiv:2210.03849.\\n\\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. 2022. Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\\n\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\\n\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu- taka Matsuo, and Yusuke Iwasawa. 2022. Large arXiv language models are zero-shot reasoners. preprint arXiv:2205.11916.\\n\\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equa- tions. Transactions of the Association for Computa- tional Linguistics, 3:585–597.\\n\\nFangyu Lei, Shizhu He, Xiang Li, Jun Zhao, and Kang Liu. 2022. Answering numerical reasoning ques- tions in table-text hybrid contents with graph-based encoder and tree-based decoder. In Proceedings of the 29th International Conference on Computational Linguistics, pages 1379–1390.\\n\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun- som. 2017. Program induction by rationale genera- tion: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158–167.\\n\\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Dynamic Clark, and Ashwin Kalyan. 2022. prompt for semi- structured mathematical reasoning. arXiv preprint arXiv:2209.14610.\\n\\nlearning via policy gradient\\n\\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and develop- ing english math word problem solvers. In Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975–984.\\n\\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical rea- soning abilities of neural models. arXiv preprint arXiv:1904.01557.\\n\\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.\\n\\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se- bastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.\\n\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle- moyer. 2022. Rethinking the role of demonstra- tions: What makes in-context learning work? arXiv preprint arXiv:2202.12837.\\n\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.\\n\\nSwaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpuro- hit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. 2022. Lila: A uniﬁed bench- mark for mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natu- ral Language Processing (EMNLP).\\n\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratch- pads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\\n\\nBin Wang, Jiangzhou Ju, Yunlin Mao, Xin-Yu Dai, Shu- jian Huang, and Jiajun Chen. 2022a. A numeri- cal reasoning question answering system with ﬁne- grained retriever and the ensemble of multiple gen- erators for ﬁnqa. arXiv preprint arXiv:2206.08506.\\n\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.\\n\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080–2094, Online. Association for Computational Linguistics.\\n\\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susan- nah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.\\n\\nSubhro Roy and Dan Roth. 2015. Solving general arith- In Proceedings of the 2015 metic word problems. Conference on Empirical Methods in Natural Lan- guage Processing, pages 1743–1752.\\n\\nSubhro Roy and Dan Roth. 2018. Mapping to declara- tive knowledge for word problem solving. Transac- tions of the Association for Computational Linguis- tics, 6:159–172.\\n\\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. In Interna- tional Conference on Learning Representations.\\n\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting enables complex reason- arXiv preprint ing in large language models. arXiv:2205.10625.\\n\\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in ﬁnance. In Proceedings of the 59th Annual Meet- ing of the Association for Computational Linguistics and the 11th International Joint Conference on Nat- ural Language Processing (Volume 1: Long Papers), pages 3277–3287.\\n\\nA Appendix\\n\\nA.1 PoT as intermediate step\\n\\nWe demonstrate the workﬂow in Figure 8. We write\\n\\nFigure 8: We adopt PoT to prompt language models to ﬁrst generate an intermediate answer and then continue to prompt large models to generate the ﬁnal answer.\\n\\nthe pseudo code as follows:\\n\\n1 # F u n c t i o n PoT ( I n p u t ) −> O u t p u t 2 # I n p u t : q u e s t i o n 3 # O u p t u t : p r o g r a m 4 # F u n c t i o n Prompt ( I n p u t ) −> O u t p u t 5 # I n p u t : q u e s t i o n + i n t e r m e d i a t e 6 # O u p t u t : a n s w e r 7 p r o g r a m = PoT ( q u e s t i o n ) 8 e x e c ( p r o g r a m ) 9\\n\\ni f\\n\\ni s i n t a n c e ( ans , d i c t ) :\\n\\n10\\n\\n11\\n\\n12\\n\\n13 14 e l s e : 15\\n\\na n s = l i s t ( x . i t e m s ( ) ) . pop ( 0 ) e x t r a = ’ a c c o r d i n g t o t h e p r o g r a m : e x t r a += a n s [ 0 ] + ’ = ’ + a n s [ 1 ] p r e d = Prompt ( q u e s t i o n + e x t r a )\\n\\np r e d = a n s\\n\\n’\\n\\n16\\n\\nr e t u r n p r e d', metadata={'source': '/tmp/tmpbmzs_hml/tmp.pdf'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1B: Splitting the Text Into Chunks\n",
        "\n",
        "#### What is it ?\n",
        "- Splitting text into chunks (often referred to as \"segmentation\" or \"chunking\") in information retrieval systems is a fundamental step.\n",
        "\n",
        "#### Why do we do it ?\n",
        "- Large documents or data sources can be unwieldy. By breaking them down into smaller, more digestible pieces, information retrieval systems can process and index the content more efficiently.\n",
        "\n",
        "#### How are we going to do it ?\n",
        "- Using RecursiveCharacterTextSplitter from LangChain\n",
        "\n",
        "\n",
        "### Recursively split by character\n",
        "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
        "\n",
        "How the text is split: by list of characters.<br>\n",
        "How the chunk size is measured: by number of characters."
      ],
      "metadata": {
        "id": "7fIbPhNgUN4k"
      },
      "id": "7fIbPhNgUN4k"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "46a91e42-37a3-497d-a084-6a893d2d1d62",
      "metadata": {
        "id": "46a91e42-37a3-497d-a084-6a893d2d1d62"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
        "all_splits = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see what the 33rd piece of splitted text looks like\n",
        "print(all_splits[33].page_content)"
      ],
      "metadata": {
        "id": "aMqizcbNUftm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eadd4e99-10a1-4d1b-b8cb-29c35e08d918"
      },
      "id": "aMqizcbNUftm",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "concatenation of task instruction, text, linearized table, and question. For conversational question answering, we simply concatenate all the dialog history in the prompt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Can you see what the 34th split looks like? Can you search for it in the document?\n",
        "\n",
        "# Enter code here"
      ],
      "metadata": {
        "id": "TGTHm7OKtKEw"
      },
      "id": "TGTHm7OKtKEw",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1C: Turning the Chunks Into Numbers\n",
        "\n",
        "We turn each chunk into a list of numbers, called \"vectors\".\n",
        "\n",
        "### Why do we need to represent Text as **vectors** ?\n",
        "Representing text as vectors, especially with methods like Word2Vec, FastText, or embeddings from models like BERT and GPT, captures the semantic meaning of the words or sentences. This allows the QA system to understand and match questions and answers based on their meaning rather than just keyword overlap.\n",
        "\n",
        "### How do we store the **vector** representation of text ?\n",
        "Using Vector Stores like Chromadb or FAISS (Facebook) or ANNOY (Spotify)\n",
        "\n",
        "### What is a vectore store ?\n",
        "In the context of QA (Question-Answering) systems, a \"vector store\" refers to a storage mechanism or database optimized for storing and retrieving high-dimensional vectors. These vectors are often representations of text data in a format that can be easily compared for similarity or used in mathematical operations.\n",
        "\n",
        "\n",
        "### In our example we will use OpenAI's \"text-embedding-ada-002\" to convert text into vectors. And ChromaDb as the vectorestore database. But do explore other options provided by LangChain"
      ],
      "metadata": {
        "id": "RF9aPiiIWfSd"
      },
      "id": "RF9aPiiIWfSd"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d0e6f70e-8b0c-47c7-b0ca-82a8091e8340",
      "metadata": {
        "id": "d0e6f70e-8b0c-47c7-b0ca-82a8091e8340"
      },
      "outputs": [],
      "source": [
        "# creating a vectorestore to put the vectorized text into\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Retrieval\n",
        "\n",
        "Representing words as vectors allows us to compare how similar one vector is to another.\n",
        "\n",
        "Given the question we will see which pieces of text does ChromaDB selects as most similar."
      ],
      "metadata": {
        "id": "idW5sSsDYKyV"
      },
      "id": "idW5sSsDYKyV"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f933268a-9088-4b6e-93c5-afc6a7596fe8",
      "metadata": {
        "id": "f933268a-9088-4b6e-93c5-afc6a7596fe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e70a16dc-a405-4412-80f3-f9e1d3a6fa9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Backend GPT-3 vs. Codex We evaluated GPT- 3 (text-davinci-002) with PoT prompting. Un- like Codex, GPT-3 is not optimized for generating programs, and one would expect degraded perfor- mance with GPT-3 as the LLM. We choose three datasets, GSM8K, SVAMP, and FinQA, to analyze the performance difference of PoT and compare that relative to CoT. We show our experimental results in Table 4. We can see that the gap be- tween Codex and GPT-3 with PoT is consistently smaller than their gap with CoT. We', metadata={'source': '/tmp/tmpbmzs_hml/tmp.pdf'}), Document(page_content='Codex PoT GPT3 PoT Codex - GPT3 (PoT)\\n\\n63.1 46.9 +16.2\\n\\n71.6 60.4 +11.2\\n\\n76.4 58.9 +7.5\\n\\n85.2 80.1 +5.1\\n\\n40.4 26.1 +14.3\\n\\n64.5 56.7 +7.8\\n\\n0.62\\n\\n0.58\\n\\n0.55\\n\\n0.65\\n\\n0.68\\n\\n0.6\\n\\n0.67\\n\\n0.69\\n\\n0.65\\n\\n0.73\\n\\n0.7\\n\\n0.74\\n\\nTable 4: GPT-3 and Codex performance difference un- der CoT and PoT prompting.\\n\\n2-shots\\n\\n0.63\\n\\n0.64\\n\\n4-shots\\n\\n0.66\\n\\n0.63\\n\\n0.62\\n\\n6-shots\\n\\n0.66\\n\\n0.64\\n\\n0.62\\n\\n8-shots\\n\\n0.64\\n\\n0.62\\n\\n0.62\\n\\nincreases as the dataset becomes more challenging.\\n\\n0.58', metadata={'source': '/tmp/tmpbmzs_hml/tmp.pdf'}), Document(page_content='3https://openai.com/blog/openai-codex/ 4https://beta.openai.com/ 5https://www.sympy.org/en/index.html\\n\\nBaselines We report results for three different models including Codex (Chen et al., 2021a), GPT-\\n\\nDataset\\n\\nSplit\\n\\nExample Domain\\n\\nInput\\n\\nOutput\\n\\nGSM8K (Cobbe et al., 2021) AQuA (Ling et al., 2017) SVAMP (Patel et al., 2021) MultiArith (Roy and Roth, 2015) TabMWP (Lu et al., 2022) FinQA (Chen et al., 2021b) ConvFinQA (Chen et al., 2022) TATQA (Zhu et al., 2021)', metadata={'source': '/tmp/tmpbmzs_hml/tmp.pdf'}), Document(page_content='Implementation Details We use the OpenAI Codex (code-davinci-002) API3 and GPT-3 (text- davinci-002) API4 model in our experiments. We use Python 3.8 with the SymPy library5 to execute the generated program. For the few-shot setting, we use 4-8 shots for all the datasets, based on their difﬁculty. For simple datasets like FinQA (Chen et al., 2021b), we tend to use less shots, while for', metadata={'source': '/tmp/tmpbmzs_hml/tmp.pdf'})]\n"
          ]
        }
      ],
      "source": [
        "# Our question. Try changing this and see what happens\n",
        "question = \"Can you compare the results of the GPT-3 backend to the Codex backend according to the paper?\"\n",
        "\n",
        "# document retrieval using ChromaDb and provided question\n",
        "docs = vectorstore.similarity_search(question)\n",
        "\n",
        "# lets see which text pieces are retrieved\n",
        "print(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell above, try asking a different question about the document, and see which docs are selected. Are they relevant to the question you asked?"
      ],
      "metadata": {
        "id": "FlB0JE3_ui_Q"
      },
      "id": "FlB0JE3_ui_Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Creating the Chain\n",
        "\n",
        "We have the question we want to ask, and have retrieved the relevant documents to answer it. How do we put this all together to generate the answer for question?"
      ],
      "metadata": {
        "id": "bSQaWHqjY71e"
      },
      "id": "bSQaWHqjY71e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2A: Specify the Language Model\n",
        "\n",
        "### How do we communicate with LLMs ?\n",
        "\n",
        "\n",
        "\n",
        "To communicate with large language models (LLMs), input a clear textual prompt or query. The model analyzes the input and generates a coherent, contextually relevant response. For optimal results, provide specific context or details within your queries\n",
        "\n",
        "\n",
        "Below we either create our own prompt or use one of the templates from LangChain. Add the question and the retrieved context and just input it to the LLM. In this case the LLM is GPT 3.5 Turbo (ChatGPT). You can play around and used other publicly available LLMs like Falcon or Llama 2 mdoels.\n"
      ],
      "metadata": {
        "id": "2YrKprOUvET4"
      },
      "id": "2YrKprOUvET4"
    },
    {
      "cell_type": "code",
      "source": [
        "# defining which LLM to use.\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "WleKRXAmeQgZ"
      },
      "id": "WleKRXAmeQgZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2B: Specify The Prompt Template"
      ],
      "metadata": {
        "id": "5M1XKGzvUZdE"
      },
      "id": "5M1XKGzvUZdE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the prompt\n",
        "template = \"\"\"Use the following pieces of a research paper to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "8jZEZ_KQeQ58"
      },
      "id": "8jZEZ_KQeQ58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2C: Putting it Together: Creating the Chain"
      ],
      "metadata": {
        "id": "X7tSn__DeYXj"
      },
      "id": "X7tSn__DeYXj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e39fd74-d723-4d31-a23b-c9162a1286fa",
      "metadata": {
        "id": "9e39fd74-d723-4d31-a23b-c9162a1286fa"
      },
      "outputs": [],
      "source": [
        "# combining all the previous steps and creating a nice and clean Chain Object.\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm, # llm we created in step 2A\n",
        "    retriever=vectorstore.as_retriever(), # vector store we created in step 1C\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT} # Prompt template we created in step 2B\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Using the Chain\n",
        "\n",
        "To use our chain, simply call the chain like a function, passing in your question."
      ],
      "metadata": {
        "id": "s9YIpYf4u4sp"
      },
      "id": "s9YIpYf4u4sp"
    },
    {
      "cell_type": "code",
      "source": [
        "# our question\n",
        "question = \"Can you provide a summary of the results mentioned in the paper?\"\n",
        "\n",
        "# getting the answer\n",
        "answer = qa_chain.run(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "z-SBA5aoq_eD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1490df96-b23a-4ecb-ec09-201c31892aa6"
      },
      "id": "z-SBA5aoq_eD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper evaluates the performance of PoT prompting on five MWP datasets (GSM8K, AQuA, SVAMP, TabMWP, MultiArith) and three financial datasets (FinQA, ConvFinQA, TATQA). PoT outperforms CoT significantly across all evaluated datasets, with an average gain of around 8% for MWP datasets and 15% for financial datasets under the few-shot setting. Thanks for asking!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Bonus) Try It Yourself - AutoGPT\n",
        "\n",
        "This langchain pipeline searches ArXiv for information, then answers your question.\n",
        "\n",
        "Notice how when you run the cell below, the AI recognizes when it does not know the answer to a question, then uses the Search function we provide it to find a paper explaining it."
      ],
      "metadata": {
        "id": "gjpnOJ7asdsf"
      },
      "id": "gjpnOJ7asdsf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ac22b3-c4af-4e68-8e19-c5ac92f517c4",
      "metadata": {
        "id": "d3ac22b3-c4af-4e68-8e19-c5ac92f517c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dd5c086b-0430-4b18-ddce-a7efcf6e1339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "arxiv: A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [arxiv]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: What is contrast consistent search and how does it help with eliciting latent knowledge?\n",
            "Thought:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "arxiv: A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [arxiv]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: What is contrast consistent search and how does it help with eliciting latent knowledge?\n",
            "Thought:I'm not familiar with the term \"contrast consistent search\" or how it relates to eliciting latent knowledge. I should use the arxiv tool to search for relevant articles that might provide more information.\n",
            "Action: arxiv\n",
            "Action Input: \"contrast consistent search eliciting latent knowledge\"\n",
            "Observation: Published: 2023-09-13\n",
            "Title: Unsupervised Contrast-Consistent Ranking with Language Models\n",
            "Authors: Niklas Stoehr, Pengxiang Cheng, Jing Wang, Daniel Preotiuc-Pietro, Rajarshi Bhowmik\n",
            "Summary: Language models contain ranking-based knowledge and are powerful solvers of\n",
            "in-context ranking tasks. For instance, they may have parametric knowledge\n",
            "about the ordering of countries by size or may be able to rank reviews by\n",
            "sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting\n",
            "techniques to elicit a language model's ranking knowledge. However, we find\n",
            "that even with careful calibration and constrained decoding, prompting-based\n",
            "techniques may not always be self-consistent in the rankings they produce. This\n",
            "motivates us to explore an alternative approach that is inspired by an\n",
            "unsupervised probing method called Contrast-Consistent Search (CCS). The idea\n",
            "is to train a probing model guided by a logical constraint: a model's\n",
            "representation of a statement and its negation must be mapped to contrastive\n",
            "true-false poles consistently across multiple statements. We hypothesize that\n",
            "similar constraints apply to ranking tasks where all items are related via\n",
            "consistent pairwise or listwise comparisons. To this end, we extend the binary\n",
            "CCS method to Contrast-Consistent Ranking (CCR) by adapting existing ranking\n",
            "methods such as the Max-Margin Loss, Triplet Loss, and Ordinal Regression\n",
            "objective. Our results confirm that, for the same language model, CCR probing\n",
            "outperforms prompting and even performs on a par with prompting much larger\n",
            "language models.\n",
            "\n",
            "Published: 2023-02-16\n",
            "Title: Neuro-Symbolic Procedural Planning with Commonsense Prompting\n",
            "Authors: Yujie Lu, Weixi Feng, Wanrong Zhu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, William Yang Wang\n",
            "Summary: Procedural planning aims to implement complex high-level goals by\n",
            "decomposition into sequential simpler low-level steps. Although procedural\n",
            "planning is a basic skill set for humans in daily life, it remains a challenge\n",
            "for large language models (LLMs) that lack a deep understanding of the\n",
            "cause-effect relations in procedures. Previous methods require manual exemplars\n",
            "to acquire procedural planning knowledge from LLMs in the zero-shot setting.\n",
            "However, such elicited pre-trained knowledge in LLMs induces spurious\n",
            "correlations between goals and steps, which impair the model generalization to\n",
            "unseen tasks. In contrast, this paper proposes a neuro-symbolic procedural\n",
            "PLANner (PLAN) that elicits procedural planning knowledge from the LLMs with\n",
            "commonsense-infused prompting. To mitigate spurious goal-step correlations, we\n",
            "use symbolic program executors on the latent procedural representations to\n",
            "formalize prompts from commonsense knowledge bases as a causal intervention\n",
            "toward the Structural Causal Model. Both automatic and human evaluations on\n",
            "WikiHow and RobotHow show the superiority of PLAN on procedural planning\n",
            "without further training or manual exemplars.\n",
            "\n",
            "Published: 2023-08-22\n",
            "Title: Simulation-Based Prior Knowledge Elicitation for Parametric Bayesian Models\n",
            "Authors: Florence Bockting, Stefan T. Radev, Paul-Christian Bürkner\n",
            "Summary: A central characteristic of Bayesian statistics is the ability to\n",
            "consistently incorporate prior knowledge into various modeling processes. In\n",
            "this paper, we focus on translating domain expert knowledge into corresponding\n",
            "prior distributions over model parameters, a process known as prior\n",
            "elicitation. Expert knowledge can manifest itself in diverse formats, including\n",
            "information about raw data, summary statistics, or model parameters. A major\n",
            "challenge for existing elicitation methods is how to effectively utilize all of\n",
            "these different formats in order to formulate prior distributions that align\n",
            "with the expert's expectations, regardless of the model structure. To address\n",
            "these challenges, we develop a simulation-based elicitation method that can\n",
            "learn the hyperparameters of potentially any parametric prior distribution from\n",
            "a wide spectrum of ex\n",
            "Thought:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Contrast consistent search\" is a method used to improve the self-consistency of rankings produced by language models. It does not directly relate to eliciting latent knowledge.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "import langchain\n",
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "\n",
        "langchain.verbose=True\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "tools=load_tools(\n",
        "    [\"arxiv\"]\n",
        ")\n",
        "\n",
        "chain = initialize_agent(\n",
        "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False #or true\n",
        ")\n",
        "\n",
        "chain.run(\"What is contrast consistent search and how does it help with eliciting latent knowledge?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8df5edd-285b-4ea0-8adb-feb5bfe57704",
      "metadata": {
        "id": "d8df5edd-285b-4ea0-8adb-feb5bfe57704"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "langchain"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}